{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks - Final Project\n",
    "\n",
    "\"Is there an optimal road or road structure to close and convert to a public use space in Barcelona that minimizes the impact on traffic?\"\n",
    "\n",
    "-Clarice Mottet, Amber Walker, Mox Ballo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Notes:\n",
    "\n",
    "There are only 930 nodes in the link_file, there are 2522 edges in the link_file (haven't checked for duplicates). There are no duplicate edges (based on origin_node to to_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "website link that the frank wolfe code is from\n",
    "\n",
    "https://nbviewer.org/github/PyTrans/Urban-Network-Analysis/blob/master/Trip_Assignment-Frank-Wolfe_Algorithm.ipynb\n",
    "\n",
    "website link that the TransportationNetworks code is from\n",
    "\n",
    "https://github.com/PyTrans/Urban-Network-Analysis/tree/master/pytrans/UrbanNetworkAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPER IMPORTANT\n",
    "\n",
    "I saw in the TransportationNetworks.py file that their initial alpha was .5 but everywhere said it should be .15, so I changed it to .15 in my file.\n",
    "\n",
    "Because there is no node_file and I have to create it, I had to alter some of the TransportationNetworks.py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.integrate as integrate \n",
    "from scipy.optimize import minimize_scalar\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import derivative\n",
    "# import TransportationNetworks as tn\n",
    "\n",
    "# import os\n",
    "# os.chdir('/content/drive/My Drive/networks_final_project_collab/networks_finalproject')\n",
    "\n",
    "import TransportationNetworks_CM as tncm\n",
    "\n",
    "# path_in_ = r'./inputs/'\n",
    "# path_out_ = r'./outputs'\n",
    "# path_out_results_ = r'./outputs/results'\n",
    "\n",
    "path_in_ = r'/home/clarice/Documents/VSCode/Term2_Networks/final_project/networks_finalproject/inputs/'\n",
    "path_out_ = r'/home/clarice/Documents/VSCode/Term2_Networks/final_project/networks_finalproject/outputs/'\n",
    "path_out_results_ = r'/home/clarice/Documents/VSCode/Term2_Networks/final_project/networks_finalproject/outputs/results/'\n",
    "path_out_social_ = r'/home/clarice/Documents/VSCode/Term2_Networks/final_project/networks_finalproject/outputs/social/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions: user, based on jupyter notebook\n",
    "\n",
    "#from website to calculate latency\n",
    "def BPR(t0, xa, ca, alpha, beta):\n",
    "    ta = t0*(1+alpha*(xa/ca)**beta)\n",
    "    return ta\n",
    "\n",
    "#from website: method to calculate nash equilibrium (when SO = False)\n",
    "def calculateZ(theta, network, SO):\n",
    "    z = 0\n",
    "    for linkKey, linkVal in network.items():\n",
    "        t0 = linkVal['t0']\n",
    "        ca = linkVal['capa']\n",
    "        beta = linkVal['beta']\n",
    "        alpha = linkVal['alpha']\n",
    "        aux = linkVal['auxiliary'][-1]\n",
    "        flow = linkVal['flow'][-1]\n",
    "        \n",
    "        if SO == False:\n",
    "            z += integrate.quad(lambda x: BPR(t0, x, ca, alpha, beta), 0, flow+theta*(aux-flow))[0]\n",
    "        elif SO == True:\n",
    "            z += list(map(lambda x : x * BPR(t0, x, ca, alpha, beta), [flow+theta*(aux-flow)]))[0]\n",
    "    return z\n",
    "\n",
    "#from website: finds nash equilibrium\n",
    "def lineSearch(network, SO):\n",
    "    theta = minimize_scalar(lambda x: calculateZ(x, network, SO), bounds = (0,1), method = 'Bounded')\n",
    "    return theta.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions: user, based on jupyter notebook\n",
    "\n",
    "def initialize(link_file, trip_file, od_vols, origins, od_dic, links, graph, SO):\n",
    "    #set objective and open the network\n",
    "    barcelonaSubset = tncm.Network(link_file, trip_file, od_vols, origins, od_dic, links, graph)\n",
    "\n",
    "    #CM - start of initialization\n",
    "\n",
    "    # define output variables, network and fwResult\n",
    "    network = {(u,v): {'t0':d['object'].t0, 'alpha':d['object'].alpha, \\\n",
    "            'beta':d['object'].beta, 'capa':d['object'].capacity, 'flow':[], \\\n",
    "            'auxiliary':[], 'cost':[]} for (u, v, d) in barcelonaSubset.graph.edges(data=True)}\n",
    "\n",
    "    fwResult = {'theta':[], 'z':[]}\n",
    "\n",
    "    # initial all-or-nothing assignment and update link travel time(cost)\n",
    "    barcelonaSubset.all_or_nothing_assignment()\n",
    "    barcelonaSubset.update_linkcost()\n",
    "\n",
    "    for linkKey, linkVal in network.items():\n",
    "        linkVal['cost'].append(barcelonaSubset.graph[linkKey[0]][linkKey[1]]['weight'])\n",
    "        linkVal['auxiliary'].append(barcelonaSubset.graph[linkKey[0]][linkKey[1]]['object'].vol)\n",
    "        linkVal['flow'].append(barcelonaSubset.graph[linkKey[0]][linkKey[1]]['object'].vol)\n",
    "\n",
    "    return barcelonaSubset, network, fwResult\n",
    "\n",
    "#frank wolfe algorithm function\n",
    "def frank_wolfe(barcelonaSubset, network, fwResult, SO):\n",
    "    ## iterations\n",
    "    iterNum=0\n",
    "    iteration = True\n",
    "    while iteration:\n",
    "        iterNum += 1\n",
    "        barcelonaSubset.all_or_nothing_assignment()\n",
    "        barcelonaSubset.update_linkcost()\n",
    "        \n",
    "        # set auxiliary flow using updated link flow\n",
    "        for linkKey, linkVal in network.items():\n",
    "            linkVal['auxiliary'].append(barcelonaSubset.graph[linkKey[0]][linkKey[1]]['object'].vol)\n",
    "            \n",
    "        # getting optimal move size (theta)\n",
    "        theta = lineSearch(network, SO)\n",
    "        fwResult['theta'].append(theta)\n",
    "        \n",
    "        # set link flow (move) based on the theta, auxiliary flow, and link flow of previous iteration\n",
    "        for linkKey, linkVal in network.items():\n",
    "            aux = linkVal['auxiliary'][-1]\n",
    "            flow = linkVal['flow'][-1]\n",
    "            linkVal['flow'].append(flow + theta*(aux-flow))\n",
    "            \n",
    "            barcelonaSubset.graph[linkKey[0]][linkKey[1]]['object'].vol =  flow + theta * (aux - flow)\n",
    "            barcelonaSubset.graph[linkKey[0]][linkKey[1]]['object'].flow = flow + theta * (aux - flow)\n",
    "            \n",
    "        # update link travel time\n",
    "        barcelonaSubset.update_linkcost()\n",
    "        \n",
    "        # calculate objective function value\n",
    "        z=0\n",
    "        for linkKey, linkVal in network.items():\n",
    "            linkVal['cost'].append(barcelonaSubset.graph[linkKey[0]][linkKey[1]]['weight'])\n",
    "            totalcost = barcelonaSubset.graph[linkKey[0]][linkKey[1]]['object'].get_objective_function()\n",
    "            z+=totalcost\n",
    "            \n",
    "        fwResult['z'].append(z)        \n",
    "            \n",
    "        # convergence test\n",
    "        if iterNum == 1:\n",
    "            iteration = True\n",
    "        else:\n",
    "            if abs(fwResult['z'][-2] - fwResult['z'][-1]) <= 0.001 or iterNum==3000:\n",
    "                iteration = False\n",
    "\n",
    "    return barcelonaSubset, network, fwResult\n",
    "\n",
    "def cash_overall(network):\n",
    "    overall_travel_time = 0.0\n",
    "    overall_total_cost = 0.0\n",
    "\n",
    "    for (u, v, d) in network.graph.edges(data=True):\n",
    "        link_obj = d['object']\n",
    "        current_flow = link_obj.flow  # Assuming this gives the final flow\n",
    "        t0 = link_obj.t0\n",
    "        c = link_obj.capacity\n",
    "        alpha = link_obj.alpha\n",
    "        beta = link_obj.beta\n",
    "        # Calculate the travel time with the final flow\n",
    "        travel_time = BPR(t0, current_flow, c, alpha, beta)\n",
    "        overall_travel_time += travel_time\n",
    "        # Optionally calculate total cost (travel time * flow)\n",
    "        total_cost = travel_time * current_flow\n",
    "        overall_total_cost += total_cost\n",
    "        # Update the network with the new travel time and total cost\n",
    "        network.graph[u][v]['travel_time'] = travel_time\n",
    "        network.graph[u][v]['total_cost'] = total_cost\n",
    "    return network, overall_travel_time, overall_total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions: user - based on TransportationNetworks\n",
    "\n",
    "def open_trip_file_CM(trip_file):\n",
    "    demand_factor=1.0\n",
    "    f = open(trip_file)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    od_vols = {}\n",
    "    current_origin = None\n",
    "\n",
    "    for line in lines:\n",
    "        if current_origin == None and line.startswith(\"Origin\"):\n",
    "            origin = str(int(line.split(\"Origin\")[1]))\n",
    "            current_origin = origin\n",
    "\n",
    "        elif current_origin != None and len(line) < 3:\n",
    "            # print \"blank\",line,\n",
    "            current_origin = None\n",
    "\n",
    "        elif current_origin != None:\n",
    "            to_process = line[0:-2]\n",
    "            for el in to_process.split(\";\"):\n",
    "                try:\n",
    "                    dest = str(int(el.split(\":\")[0]))\n",
    "                    demand = float(el.split(\":\")[1]) * demand_factor\n",
    "                    od_vols[current_origin, dest] = demand\n",
    "                except:\n",
    "                    continue\n",
    "    origins = [str(i) for i, j in od_vols]\n",
    "    origins = list(dict.fromkeys(origins).keys())\n",
    "\n",
    "    od_dic = {}\n",
    "    for (origin, destination) in od_vols:\n",
    "        if origin not in od_dic:\n",
    "            od_dic[origin] = {}\n",
    "\n",
    "        od_dic[origin][destination] = od_vols[origin, destination]\n",
    "    return od_vols, origins, od_dic\n",
    "\n",
    "\n",
    "def open_link_file_CM(link_file, SO):\n",
    "    link_fields = {\"from\": 1, \"to\": 2, \"capacity\": 3, \"length\": 4, \"t0\": 5, \\\n",
    "                    \"B\": 6, \"beta\": 7, \"V\": 8}\n",
    "    f = open(link_file)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    links_info = []\n",
    "\n",
    "    header_found = False\n",
    "    for line in lines:\n",
    "        if not header_found and line.startswith(\"~\"):\n",
    "            header_found = True\n",
    "        elif header_found:\n",
    "            links_info.append(line)\n",
    "\n",
    "    nodes = {}\n",
    "    links = []\n",
    "\n",
    "    for line in links_info:\n",
    "        data = line.split(\"\\t\")\n",
    "\n",
    "        try:\n",
    "            origin_node = str(int(data[link_fields[\"from\"]]))\n",
    "        except IndexError:\n",
    "            continue\n",
    "        to_node = str(int(data[link_fields[\"to\"]]))\n",
    "        capacity = float(data[link_fields[\"capacity\"]])\n",
    "        length = float(data[link_fields[\"length\"]])\n",
    "        alpha = float(data[link_fields[\"B\"]])\n",
    "        beta = float(data[link_fields[\"beta\"]])\n",
    "\n",
    "        if origin_node not in nodes:\n",
    "            n = tncm.Node(node_id=origin_node)\n",
    "            nodes[origin_node] = n\n",
    "\n",
    "        if to_node not in nodes:\n",
    "            n = tncm.Node(node_id=to_node)\n",
    "            nodes[to_node] = n\n",
    "\n",
    "        l = tncm.Link(link_id=len(links), length=length, capacity=capacity, alpha=alpha, beta=beta,\n",
    "                    from_node=origin_node, to_node=to_node, flow=float(0.0), SO=SO)\n",
    "\n",
    "        links.append(l)\n",
    "    return links\n",
    "    \n",
    "def build_datastructure_CM(links):        \n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    for l in links:\n",
    "        graph.add_edge(l.get_from_node(), l.get_to_node(), object=l, time=l.get_time())\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions: user, scratch\n",
    "\n",
    "#used in the identify superblocks code\n",
    "def common_rmv_round(graph, neighbors_node_, common_node):\n",
    "    for node_ in neighbors_node_:\n",
    "        neighbors_node__ = list(graph.neighbors(node_))\n",
    "        for node_common in common_node:\n",
    "            if node_common in neighbors_node__:\n",
    "                common_node.append(node_)\n",
    "                common_node = list(set(list(common_node)))\n",
    "                if node_ in neighbors_node_:\n",
    "                    neighbors_node_.remove(node_)        \n",
    "    return neighbors_node_, common_node\n",
    "\n",
    "\n",
    "def identify_superblocks_directed(graph_directed, dict_blocks):\n",
    "    dict_blocks_directed = {}\n",
    "\n",
    "    for iter_ in dict_blocks.keys():\n",
    "        list_center = dict_blocks[iter_][0]\n",
    "        list_corner = dict_blocks[iter_][1]\n",
    "        list_outside = dict_blocks[iter_][2]\n",
    "\n",
    "        #make sure that removing the edges to the center doesn't cause issues to the outside nodes\n",
    "        #like if an outside node is only degree 2 and then one edge is removed and its an origin node\n",
    "        #so the cars can't leave the node\n",
    "        dict_outside_degree = {}\n",
    "        for node_center in list_center:\n",
    "            dict_outside_degree[node_center] = []\n",
    "            for node in list_outside:\n",
    "                edge_in_exists = graph_directed.has_edge(node_center, node)\n",
    "                edge_out_exists = graph_directed.has_edge(node, node_center)\n",
    "                if ((graph_directed.in_degree(node)-int(edge_in_exists)) >= 1)&((graph_directed.out_degree(node)-int(edge_out_exists)) >= 1):\n",
    "                    dict_outside_degree[node_center].append(node)\n",
    "        \n",
    "        outside_clear = 0\n",
    "        for node_center in list_center:\n",
    "            if len(dict_outside_degree[node_center])==len(list_outside):\n",
    "                outside_clear += 1\n",
    "                \n",
    "        list_corner_degree = []\n",
    "        #make sure that there are at least two nodes on the corners that have\n",
    "        #in degree > 1 and out degree > 1\n",
    "        for node in list_corner:\n",
    "            if (graph_directed.in_degree(node) >= 1)&(graph_directed.out_degree(node) >= 1):\n",
    "                list_corner_degree.append(node)\n",
    "\n",
    "        if (len(list_corner_degree) >= 2)&(outside_clear==4):\n",
    "            dict_blocks_directed[iter_] = [list_center, list_corner, list_outside, list_corner_degree]\n",
    "\n",
    "    return dict_blocks_directed\n",
    "\n",
    "\n",
    "#function that identifies superblocks in an undirected graph\n",
    "def identify_superblocks(graph_directed):\n",
    "    graph = nx.Graph(graph_directed)\n",
    "    list_nodes = list(graph.nodes)\n",
    "\n",
    "    dict_blocks = {}\n",
    "    iter_ = 0\n",
    "\n",
    "    for node_i in list_nodes:\n",
    "        # for node_i in list_nodes:\n",
    "        neighbors_i = list(graph.neighbors(node_i))\n",
    "        neighbors_i_ = list(graph.neighbors(node_i))\n",
    "        nodes_common = []\n",
    "        for node_j in neighbors_i:\n",
    "            neighbors_j = list(graph.neighbors(node_j))\n",
    "            neighbors_j.remove(node_i)\n",
    "            neighbors_i_.remove(node_j)\n",
    "            #make sure that you remove common neighbors in neighbors_j\n",
    "            for node_j_ in neighbors_i:\n",
    "                if node_j != node_j_:\n",
    "                    neighbors_j_ = list(graph.neighbors(node_j_))\n",
    "                    for element in neighbors_j_:\n",
    "                        if element in neighbors_j:\n",
    "                            neighbors_j.remove(element)\n",
    "                            nodes_common.append(element)\n",
    "                            nodes_common = list(set(list(nodes_common)))\n",
    "            #continue to next node\n",
    "            for node_k in neighbors_j:\n",
    "                neighbors_k = list(graph.neighbors(node_k))\n",
    "                neighbors_k.remove(node_j)\n",
    "                #remove common neighbors\n",
    "                neighbors_k, nodes_common = common_rmv_round(graph, neighbors_k, nodes_common)\n",
    "                for node_l in neighbors_k:\n",
    "                    neighbors_l = list(graph.neighbors(node_l))\n",
    "                    neighbors_l.remove(node_k)\n",
    "                    #now we have (node_i, node_j, node_k, node_l)\n",
    "                    #go to row 2\n",
    "                    for node_m in neighbors_i_:\n",
    "                        #we don't want node_j to equal node_m\n",
    "                        neighbors_m = list(graph.neighbors(node_m))\n",
    "                        neighbors_m.remove(node_i)\n",
    "                        for node_n in neighbors_m:\n",
    "                            neighbors_n = list(graph.neighbors(node_n))\n",
    "                            neighbors_n.remove(node_m)\n",
    "                            for node_o in neighbors_n:\n",
    "                                if node_o != node_j:\n",
    "                                    neighbors_o = list(graph.neighbors(node_o))\n",
    "                                    neighbors_o.remove(node_n)\n",
    "                                    for node_p in neighbors_o:\n",
    "                                        if node_p != node_k:\n",
    "                                            neighbors_p = list(graph.neighbors(node_p))\n",
    "                                            neighbors_p.remove(node_o)\n",
    "                                            #now we have (node_m, node_n, node_o, node_p)\n",
    "                                            #if first row is connected to second row\n",
    "                                            if (node_j in neighbors_n)&(node_k in neighbors_o)&(node_l in neighbors_p):\n",
    "                                                #continue to third row\n",
    "                                                for node_q in neighbors_m:\n",
    "                                                    neighbors_q = list(graph.neighbors(node_q))\n",
    "                                                    neighbors_q.remove(node_m)\n",
    "                                                    for node_r in neighbors_q:\n",
    "                                                        neighbors_r = list(graph.neighbors(node_r))\n",
    "                                                        neighbors_r.remove(node_q)\n",
    "                                                        for node_s in neighbors_r:\n",
    "                                                            if node_s != node_n:\n",
    "                                                                neighbors_s = list(graph.neighbors(node_s))\n",
    "                                                                neighbors_s.remove(node_r)\n",
    "                                                                for node_t in neighbors_s:\n",
    "                                                                    if node_t !=node_o:\n",
    "                                                                        neighbors_t = list(graph.neighbors(node_t))\n",
    "                                                                        neighbors_t.remove(node_s)\n",
    "                                                                        if (node_n in neighbors_r)&(node_o in neighbors_s)&(node_p in neighbors_t):\n",
    "                                                                            #now we have (node_q, node_r, node_s, node_t)\n",
    "                                                                            # print(\"third row\",node_q, node_r, node_s, node_t)\n",
    "                                                                            for node_w in neighbors_q:\n",
    "                                                                                neighbors_w = list(graph.neighbors(node_w))\n",
    "                                                                                neighbors_w.remove(node_q)\n",
    "                                                                                for node_x in neighbors_w:\n",
    "                                                                                    neighbors_x = list(graph.neighbors(node_x))\n",
    "                                                                                    neighbors_x.remove(node_w)\n",
    "                                                                                    for node_y in neighbors_x:\n",
    "                                                                                        neighbors_y = list(graph.neighbors(node_y))\n",
    "                                                                                        neighbors_y.remove(node_x)\n",
    "                                                                                        for node_z in neighbors_y:\n",
    "                                                                                            if node_z != node_s:\n",
    "                                                                                                neighbors_z = list(graph.neighbors(node_z))\n",
    "                                                                                                neighbors_z.remove(node_y)\n",
    "                                                                                                if (node_r in neighbors_x)&(node_s in neighbors_y)&(node_t in neighbors_z):\n",
    "                                                                                                    #document all the nodes                                                                \n",
    "                                                                                                    list_center = [node_n, node_o, node_r, node_s]\n",
    "                                                                                                    list_corner = [node_i, node_l, node_w, node_z]\n",
    "                                                                                                    list_outside = [node_i, node_j, node_k, node_l, node_m, node_p, node_q, node_t, node_w, node_x, node_y, node_z]\n",
    "\n",
    "                                                                                                    list_center.sort()\n",
    "                                                                                                    list_corner.sort()\n",
    "                                                                                                    list_outside.sort()\n",
    "\n",
    "                                                                                                    #make sure no duplicate nodes    \n",
    "                                                                                                    list_center = list(set(list_center))\n",
    "                                                                                                    list_outside = list(set(list_outside))\n",
    "\n",
    "                                                                                                    #check that all nodes are degree 4\n",
    "                                                                                                    all_nodes_degree4 = 0\n",
    "                                                                                                    # for node in list_outside:\n",
    "                                                                                                    #     if graph.degree[node] > 3:\n",
    "                                                                                                    #         all_nodes_degree4 += 1\n",
    "                                                                                                    for node in list_center:\n",
    "                                                                                                        if graph.degree[node] == 4:\n",
    "                                                                                                            all_nodes_degree4 += 1\n",
    "                                                                                                    \n",
    "                                                                                                    if all_nodes_degree4 == 4:\n",
    "                                                                                                        # print(\"all nodes are degree 4\")\n",
    "                                                                                                        #check to make sure that the same graph isn't being added again\n",
    "                                                                                                        are_equal = 0\n",
    "                                                                                                        for prev in dict_blocks.keys():\n",
    "                                                                                                            if (dict_blocks[prev][0] == list_center)&(dict_blocks[prev][2] == list_outside):\n",
    "                                                                                                                are_equal += 1\n",
    "                                                                                                        if are_equal == 0:\n",
    "                                                                                                            #add the new superblock to the dictionary\n",
    "                                                                                                            # print(iter_)\n",
    "                                                                                                            dict_blocks[iter_] = [list_center, list_corner, list_outside]\n",
    "                                                                                                            iter_ += 1\n",
    "    print(\"Number of superblocks\",len(dict_blocks.keys()))\n",
    "    dict_blocks_directed = identify_superblocks_directed(graph_directed, dict_blocks)\n",
    "    print(\"Number of superblocks directed\",len(dict_blocks_directed.keys()))\n",
    "    return dict_blocks_directed, dict_blocks\n",
    "\n",
    "#this is code that updates the trips file by removing a superblock\n",
    "def update_od_vols(list_dict_blocks_directed_iter, od_vols):\n",
    "    list_center = list_dict_blocks_directed_iter[0]\n",
    "    list_corner_degree = list_dict_blocks_directed_iter[3]\n",
    "\n",
    "    in_corner_node = list_corner_degree[0]\n",
    "    out_corner_node = list_corner_degree[1]\n",
    "\n",
    "    #make a copy\n",
    "    od_vols_out = {}\n",
    "    for key_ in od_vols.keys():\n",
    "        od_vols_out[key_] = od_vols[key_]\n",
    "\n",
    "    #compile the relevant \n",
    "    origin_demand_center = []\n",
    "    dest_demand_center = []\n",
    "    for key_ in od_vols_out.keys():\n",
    "        for node_center in list_center:\n",
    "            #compile the center starting nodes\n",
    "            if str(node_center) == key_[0]:\n",
    "                origin_demand_center.append(key_)\n",
    "            #compile the in node replacement\n",
    "            if str(node_center) == key_[1]:\n",
    "                dest_demand_center.append(key_)\n",
    "\n",
    "    #update the origin connections\n",
    "    for connection in origin_demand_center:\n",
    "        #create an update connection\n",
    "        if (str(in_corner_node) == connection[1])|(int(connection[1]) in list_center):\n",
    "            update_connection = (str(in_corner_node),str(out_corner_node))\n",
    "        else:\n",
    "            update_connection = (str(in_corner_node),connection[1])\n",
    "        if update_connection in list(od_vols_out.keys()):\n",
    "            od_vols_out[update_connection] += od_vols_out[connection]\n",
    "        else:\n",
    "            od_vols_out[update_connection] = od_vols[connection]\n",
    "        del od_vols_out[connection]\n",
    "\n",
    "    for connection in dest_demand_center:\n",
    "        if connection not in origin_demand_center:\n",
    "            #create an update connection\n",
    "            if (str(out_corner_node) == connection[0])|(int(connection[0]) in list_center):\n",
    "                update_connection = (str(in_corner_node),str(out_corner_node))\n",
    "            else:\n",
    "                update_connection = (connection[0], str(out_corner_node))\n",
    "            if update_connection in list(od_vols_out.keys()):\n",
    "                od_vols_out[update_connection] += od_vols_out[connection]\n",
    "            else:\n",
    "                od_vols_out[update_connection] = od_vols_out[connection]\n",
    "            del od_vols_out[connection]\n",
    "\n",
    "    return od_vols_out\n",
    "\n",
    "def update_trip(list_dict_blocks_directed_iter, trip_file):\n",
    "    od_vols, origins, od_dic = open_trip_file_CM(trip_file)\n",
    "    od_vols_out = update_od_vols(list_dict_blocks_directed_iter, od_vols)\n",
    "\n",
    "    del origins, od_dic\n",
    "\n",
    "    origins = [str(i) for i, j in od_vols_out]\n",
    "    origins = list(dict.fromkeys(origins).keys())\n",
    "\n",
    "    od_dic = {}\n",
    "    for (origin, destination) in od_vols_out:\n",
    "        if origin not in od_dic:\n",
    "            od_dic[origin] = {}\n",
    "\n",
    "        od_dic[origin][destination] = od_vols_out[origin, destination]\n",
    "    return od_vols_out, origins, od_dic\n",
    "\n",
    "#remove edges of the center nodes of the super block\n",
    "def update_links(list_dict_blocks_directed_iter, link_file, SO):\n",
    "    links_out = open_link_file_CM(link_file, SO)\n",
    "    list_center = list_dict_blocks_directed_iter[0]\n",
    "\n",
    "    #delete all edges tied to the center nodes of the superblock\n",
    "    remove_link_index = []\n",
    "\n",
    "    for i, l in enumerate(links_out):\n",
    "        if (l.get_from_node() in list_center)|(l.get_to_node() in list_center):\n",
    "            remove_link_index.append(i)\n",
    "\n",
    "    remove_link_index.sort(reverse = True)\n",
    "    for i in remove_link_index:\n",
    "        del links_out[i]\n",
    "    \n",
    "    return links_out\n",
    "\n",
    "def graph_w_superblock(graph, list_center, list_outside, seed):\n",
    "    # Prepare node colors\n",
    "    node_colors = []\n",
    "    for node in graph.nodes():\n",
    "        if node in list_center:\n",
    "            node_colors.append('green')\n",
    "        elif node in list_outside:\n",
    "            node_colors.append('purple') \n",
    "        else:\n",
    "            node_colors.append('blue')\n",
    "\n",
    "    # Generate a layout\n",
    "    pos = nx.spring_layout(graph, scale=1, seed = seed)\n",
    "    # pos = nx.spring_layout(graph, scale=2, seed = 42)\n",
    "\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # plt.figure(figsize=(20, 20))\n",
    "\n",
    "    # Draw the graph with specified node colors\n",
    "    nx.draw_networkx(graph, pos, arrows=True, node_size=20, width=0.1, with_labels=False, alpha=0.7, node_color=node_colors)\n",
    "\n",
    "    # Adjust plot\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data into programming environment\n",
    "directory = path_in_\n",
    "link_file = '{}Barcelona_net.tntp'.format(path_in_)\n",
    "trip_file = '{}Barcelona_trips.tntp'.format(path_in_)\n",
    "\n",
    "od_vols, origins, od_dic = open_trip_file_CM(trip_file)\n",
    "links = open_link_file_CM(link_file, False)\n",
    "graph = build_datastructure_CM(links)\n",
    "#to be able to access stuff in graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of superblocks 87\n",
      "Number of superblocks directed 87\n"
     ]
    }
   ],
   "source": [
    "#Identify all eligible superblocks in the barcelona network\n",
    "\n",
    "dict_superblocks, dict_blocks_ignore = identify_superblocks(graph)\n",
    "\n",
    "df_superblocks = pd.DataFrame(dict_superblocks)\n",
    "df_superblocks = df_superblocks.transpose()\n",
    "df_superblocks.reset_index(inplace = True)\n",
    "df_superblocks.columns = ['iter_','list_center','list_corner','list_outside','list_corner_degree']\n",
    "# df_superblocks.to_excel(path_out_+'df_superblocks.xlsx', index = False)\n",
    "\n",
    "df_blocks_ignore = pd.DataFrame(dict_blocks_ignore)\n",
    "df_blocks_ignore = df_blocks_ignore.transpose()\n",
    "df_blocks_ignore.reset_index(inplace = True)\n",
    "df_blocks_ignore.columns = ['iter_','list_center','list_corner','list_outside']\n",
    "# df_blocks_ignore.to_excel(path_out_+'df_blocks_all.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes in origin 2\n",
      "number of nodes as dest 2\n",
      "nodes in origin [53, 37]\n",
      "nodes as dest [53, 37]\n",
      "distinct number of origin nodes 97\n",
      "distinct number of destination nodes 108\n",
      "origin and dest overlap 97\n"
     ]
    }
   ],
   "source": [
    "list_origins = []\n",
    "list_dest = []\n",
    "\n",
    "for iter_ in dict_superblocks.keys():\n",
    "    list_center = dict_superblocks[iter_][2]\n",
    "    for node in list_center:\n",
    "        # print(str(node))\n",
    "        if str(node) in origins:\n",
    "            list_origins.append(node)\n",
    "        for key_ in od_dic.keys():\n",
    "            list_dest_frm_origin = od_dic[key_].keys()\n",
    "            if str(node) in list_dest_frm_origin:\n",
    "                list_dest.append(node)\n",
    "\n",
    "list_origins = list(set(list_origins))\n",
    "list_dest = list(set(list_dest))\n",
    "print(\"number of nodes in origin\",len(list_origins))\n",
    "print(\"number of nodes as dest\",len(list_dest))\n",
    "print(\"nodes in origin\",(list_origins))\n",
    "print(\"nodes as dest\",(list_dest))\n",
    "\n",
    "print(\"distinct number of origin nodes\",len(origins))\n",
    "dist_destination_nodes = []\n",
    "for key_ in od_dic.keys():\n",
    "    for node_ in od_dic[key_].keys():\n",
    "        dist_destination_nodes.append(node_)\n",
    "dist_destination_nodes = list(set(dist_destination_nodes))\n",
    "print(\"distinct number of destination nodes\",len(dist_destination_nodes))\n",
    "\n",
    "origin_and_dest = []\n",
    "for node_ in origins:\n",
    "    if node_ in dist_destination_nodes:\n",
    "        origin_and_dest.append(node_)\n",
    "print(\"origin and dest overlap\",len(origin_and_dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of center nodes 348\n",
      "number of center nodes 167\n"
     ]
    }
   ],
   "source": [
    "dist_number_of_center = []\n",
    "for iter_ in dict_superblocks.keys():\n",
    "    list_center = dict_superblocks[iter_][0]\n",
    "    for node in list_center:\n",
    "        dist_number_of_center.append(node)\n",
    "print(\"number of center nodes\",len(dist_number_of_center))\n",
    "dist_number_of_center = list(set(dist_number_of_center))\n",
    "print(\"number of center nodes\",len(dist_number_of_center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of outside nodes 1044\n",
      "number of outside nodes 322\n"
     ]
    }
   ],
   "source": [
    "dist_number_of_outside = []\n",
    "for iter_ in dict_superblocks.keys():\n",
    "    list_center = dict_superblocks[iter_][2]\n",
    "    for node in list_center:\n",
    "        dist_number_of_outside.append(node)\n",
    "print(\"number of outside nodes\",len(dist_number_of_outside))\n",
    "dist_number_of_outside = list(set(dist_number_of_outside))\n",
    "print(\"number of outside nodes\",len(dist_number_of_outside))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_remove = []\n",
    "\n",
    "for node_ in dist_number_of_center:\n",
    "    for i, node_out in enumerate(dist_number_of_outside):\n",
    "        if node_ == node_out:\n",
    "            index_remove.append(i)\n",
    "\n",
    "index_remove.sort(reverse = True)\n",
    "for i in index_remove:\n",
    "    del dist_number_of_outside[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_center = dist_number_of_center\n",
    "list_outside = dist_number_of_outside\n",
    "\n",
    "# Initialize a set with your nodes of interest\n",
    "all_nodes = set(list_center)\n",
    "\n",
    "# Add all predecessors and successors of the nodes of interest to the set\n",
    "for node in list_center:\n",
    "    all_nodes.update(graph.predecessors(node))\n",
    "    all_nodes.update(graph.successors(node))\n",
    "\n",
    "# Now, create a subgraph with all these nodes\n",
    "# subgraph = graph.subgraph(all_nodes).copy()\n",
    "\n",
    "graph_w_superblock(graph, list_center, list_outside, 60)\n",
    "# graph_w_superblock(graph, list_center, list_outside, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'graph' is your original directed graph\n",
    "# And you want a subgraph with these nodes and their connections\n",
    "nodes_of_interest = [731, 732, 685, 686]\n",
    "list_center = nodes_of_interest\n",
    "list_outside = [736, 737, 679, 680, 681, 684, 687, 660, 727, 728, 730, 733]\n",
    "\n",
    "# Initialize a set with your nodes of interest\n",
    "all_nodes = set(nodes_of_interest)\n",
    "\n",
    "# Add all predecessors and successors of the nodes of interest to the set\n",
    "for node in nodes_of_interest:\n",
    "    all_nodes.update(graph.predecessors(node))\n",
    "    all_nodes.update(graph.successors(node))\n",
    "\n",
    "# Now, create a subgraph with all these nodes\n",
    "subgraph = graph.subgraph(all_nodes).copy()\n",
    "\n",
    "graph_w_superblock(subgraph, list_center, list_outside, 60)\n",
    "# graph_w_superblock(graph, list_center, list_outside, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the total cost for the graph on the whole - nash equilibrium\n",
    "\n",
    "#initialize graph\n",
    "barcelonaSubset, network, fwResult = initialize(link_file, trip_file, od_vols, origins, od_dic, links, graph, False)\n",
    "\n",
    "#use frank-wolfe to find nash equilibrium\n",
    "barcelonaSubset, network, fwResult = frank_wolfe(barcelonaSubset, network, fwResult, False)\n",
    "\n",
    "#calculate latency\n",
    "barcelonaSubset, overall_travel_time, overall_total_cost = cash_overall(barcelonaSubset)\n",
    "\n",
    "df_out = pd.DataFrame([[-1, overall_travel_time, overall_total_cost]], columns = ['block_iter','overall_travel_time','overall_total_cost'])\n",
    "df_out.to_excel(path_out_results_+'df_cost_base'+'.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run through dictionary of superblocks in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_superblocks\n",
    "\n",
    "list_ran = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re-work the trip file so that it doesn't include any of the center nodes from any of the graphs so the origin file stays the same for every iteration but the links will have to update depending on what super block is left in or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "#for loop through the dictionary of super blocks\n",
    "SO = False\n",
    "\n",
    "# dict_superblocks.keys()\n",
    "for iter_ in dict_superblocks.keys():\n",
    "    print(iter_)\n",
    "    if iter_ not in list_ran:\n",
    "        #remove the super block from the graph\n",
    "        od_vols_out, origins_out, od_dic_out = update_trip(dict_superblocks[iter_], trip_file)\n",
    "        links_out = update_links(dict_superblocks[iter_], link_file, SO)\n",
    "\n",
    "        #initialize graph\n",
    "        barcelonaSubset, network, fwResult = initialize(link_file, trip_file, od_vols_out, origins_out, od_dic_out, links_out, graph, SO)\n",
    "\n",
    "        #use frank-wolfe to find nash equilibrium\n",
    "        barcelonaSubset, network, fwResult = frank_wolfe(barcelonaSubset, network, fwResult, SO)\n",
    "\n",
    "        #calculate latency\n",
    "        barcelonaSubset, overall_travel_time, overall_total_cost = cash_overall(barcelonaSubset)\n",
    "\n",
    "        df_out = pd.DataFrame([[iter_, overall_travel_time, overall_total_cost]], columns = ['block_iter','overall_travel_time','overall_total_cost'])\n",
    "        df_out.to_excel(path_out_results_+'df_nash_cost_'+str(iter_)+'.xlsx', index = False)\n",
    "        list_ran.append(iter_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m barcelonaSubset, network, fwResult \u001b[38;5;241m=\u001b[39m initialize(link_file, trip_file, od_vols, origins, od_dic, links, graph, SO)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#use frank-wolfe to find nash equilibrium\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m barcelonaSubset, network, fwResult \u001b[38;5;241m=\u001b[39m \u001b[43mfrank_wolfe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbarcelonaSubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwResult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#calculate latency\u001b[39;00m\n\u001b[1;32m     11\u001b[0m barcelonaSubset, overall_travel_time, overall_total_cost \u001b[38;5;241m=\u001b[39m cash_overall(barcelonaSubset)\n",
      "Cell \u001b[0;32mIn[41], line 34\u001b[0m, in \u001b[0;36mfrank_wolfe\u001b[0;34m(barcelonaSubset, network, fwResult, SO)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m iteration:\n\u001b[1;32m     33\u001b[0m     iterNum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mbarcelonaSubset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_or_nothing_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     barcelonaSubset\u001b[38;5;241m.\u001b[39mupdate_linkcost()\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# set auxiliary flow using updated link flow\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VSCode/Term2_Networks/final_project/networks_finalproject/TransportationNetworks_CM.py:394\u001b[0m, in \u001b[0;36mNetwork.all_or_nothing_assignment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(path) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    393\u001b[0m     fnode, tnode \u001b[38;5;241m=\u001b[39m path[p], path[p \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph[fnode][tnode][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvol \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m odvol\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate the total cost for the graph on the whole - social equilibrium\n",
    "SO = True\n",
    "\n",
    "#initialize graph\n",
    "barcelonaSubset, network, fwResult = initialize(link_file, trip_file, od_vols, origins, od_dic, links, graph, SO)\n",
    "\n",
    "#use frank-wolfe to find nash equilibrium\n",
    "barcelonaSubset, network, fwResult = frank_wolfe(barcelonaSubset, network, fwResult, SO)\n",
    "\n",
    "#calculate latency\n",
    "barcelonaSubset, overall_travel_time, overall_total_cost = cash_overall(barcelonaSubset)\n",
    "\n",
    "df_out = pd.DataFrame([[-1, overall_travel_time, overall_total_cost]], columns = ['block_iter','overall_travel_time','overall_total_cost'])\n",
    "df_out.to_excel(path_out_social_+'df_cost_base_social_'+'.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ran = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SO = True\n",
    "\n",
    "# dict_superblocks.keys() - social equilibrium\n",
    "for iter_ in dict_superblocks.keys():\n",
    "    print(iter_)\n",
    "    if iter_ not in list_ran:\n",
    "        #remove the super block from the graph\n",
    "        od_vols_out, origins_out, od_dic_out = update_trip(dict_superblocks[iter_], trip_file)\n",
    "        links_out = update_links(dict_superblocks[iter_], link_file, SO)\n",
    "\n",
    "        #initialize graph\n",
    "        barcelonaSubset, network, fwResult = initialize(link_file, trip_file, od_vols_out, origins_out, od_dic_out, links_out, graph, SO)\n",
    "\n",
    "        #use frank-wolfe to find nash equilibrium\n",
    "        barcelonaSubset, network, fwResult = frank_wolfe(barcelonaSubset, network, fwResult, SO)\n",
    "\n",
    "        #calculate latency\n",
    "        barcelonaSubset, overall_travel_time, overall_total_cost = cash_overall(barcelonaSubset)\n",
    "\n",
    "        df_out = pd.DataFrame([[iter_, overall_travel_time, overall_total_cost]], columns = ['block_iter','overall_travel_time','overall_total_cost'])\n",
    "        df_out.to_excel(path_out_social_+'df_social_cost_'+str(iter_)+'.xlsx', index = False)\n",
    "        list_ran.append(iter_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
